**Rules of big O notation**
Big O notation describes the upper bound of an algorithm's time complexity

## Ignoring constants
- Coefficient and lower-order terms are irrelevant for larger input sizes
- When simplifiying the coefficient don't matter
    E.g;O(3n)simplifies to O(n)
        O(n+5)simplifies to O(n)

## Focusing on the Dominant Terms
- Only the terms with the highest growth rates matter
- Retain the terms with the highest growth rates
    E.g;O(n3+n2) becomes O(n3)
        O(n2+n) becomes O(n2)

## Iterative Loops
- Nested and single loops determine complexity
- E.g-Single loop interating n times is O(n)
      -Nested loops multiply complexity O(n*n) becomes O(n2)

## Consecutive Operations Add
- If operations run sequentially, add their comlexities, then simplyfy
- E.g; O(n)+O(n2) becomes O(n2)

## Recursive calls multiply
- Analyze recursive algorithms using recurrence relations 
- E.g; A binary search splits input in half each time O(Logn)
- or A merge sort splits array into half and merges resulting in O(nLogn)

## Drop Non-Dominant Terms
- For combined complexities, retain only the highest terms
- E.g; O(n3+n2+n_logn) becomes O(n3) since it is the highest
      - O(n10+n5+n2) becomes O(n10) 

## Constant Time Operations
- Simple operations like accessing an array element are O(1)

## Multiplication for Nested Loops
- If loops depend on input sizes, multiply complexities.
- E.g; for(i=o; i<n; i++){
   for (j=o; j<n j++)
    for(j=o; j<n; j++) 
}
  becomes O(n2).

## Amortized Complexity
- Account for occasional expensive operation spread across multiple inputs
- E.g;Resizing on array backed list O(1) on average 

## Logarithm growth 
- Algorithms that divide the problem (binary search) are
O(Logn)

